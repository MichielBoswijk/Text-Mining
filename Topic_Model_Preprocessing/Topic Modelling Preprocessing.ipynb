{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling Preprocessing\n",
    "\n",
    "\n",
    "This notebook queries PubMed for a set of 24 subcortical structures reported in fMRI, PET or electrophysiology papers. It obtains the corresponding papers from a local folder. To see how the folder with abstracts was obtained please see (reference). Next, it preprocesses the abstracts so that the text is tokenized, converted to lowercase, and so that structure and function words n-grams are replaced by their underscore counterparts. It then creates 4 pandas DataFrames:\n",
    "\n",
    "  - One vocabulary with all unique words and an index\n",
    "  - One containing structures and their index in the vocabulary\n",
    "  - One containing functions and their index in the vocabulary\n",
    "  - One containing the counts of every word per document id (PubMed ID), and the index of these words.\n",
    "  \n",
    "The notebook requires that the python classes StructureInfo, PubmedReader and AbstractFormatter are in your working directory. Additionally it requires the file SubcorticalStructres.xlsx (containing all information on the structures), and as mentioned before, a folder where the abstracts are saved per structure. Finally, the packages used can be seen in the cell below.  \n",
    "\n",
    "Python 2.7.12 was used to test the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import classes\n",
    "from StructureInfo import *\n",
    "from PubmedReader import *\n",
    "from AbstractFormatter import *\n",
    "# import external libraries\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import operator\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mode number -1 = only the abstract texts\n",
    "mode_number = -1\n",
    "# template that is used to query PubMed\n",
    "query_template = '\\\"{}\\\"' + ' AND (fMRI OR PET OR electrophysiology) AND \\\"humans\\\"[MeSH Terms]'\n",
    "# local location where all abstract information is stored (.txt file per structure)\n",
    "location = '/home/michiel/Desktop/Abstracts/'\n",
    "# number of structures extracted from the list of 33 nuclei in file SubcorticalStructures.xlsx\n",
    "n_structures = 24\n",
    "# boolean indicating whether a list of cognitive function words is used to filter results\n",
    "word_list = False\n",
    "# obtain structures\n",
    "info = StructureInfo()\n",
    "structures = info.structures[0:n_structures]\n",
    "# read and process function words\n",
    "function_words = open('function_words.txt', 'r').read().split('\\n')[:-1]\n",
    "function_words = [w.lower() for w in function_words]\n",
    "# obtain list of stopwords from NLTK package\n",
    "stopword_list = [str(w) for w in stopwords.words('english')]\n",
    "# define extra stopwords to ignore\n",
    "extra_stopwords = ['objective', 'setting', 'results', 'conclusions', 'case', 'presentation', 'methods', \n",
    "                   'purpose', 'discussions', 'object', 'aim']\n",
    "# combine to create one list of stopwords\n",
    "stopword_list += extra_stopwords\n",
    "# word count cutoff point\n",
    "mininal_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # ignore punctuation except underscore\n",
    "    punct = string.punctuation.replace('_', '')\n",
    "    text = \"\".join([ch for ch in text if ch not in punct])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def preprocess(input_abstract, structures, function_words):\n",
    "    # make sure sturctures are lower case\n",
    "    structures = [s.lower() for s in structures]\n",
    "    # make abstracts lowercase\n",
    "    abstract = input_abstract.lower()  \n",
    "    # replace structures by underscore counterpart (bi/tri-grams)\n",
    "    for struct in structures:\n",
    "        if struct in abstract:\n",
    "            abstract = abstract.replace(struct, struct.replace(' ', '_'))\n",
    "    # replace function words by underscore counterpart (bi/tri-grams)\n",
    "    for function in function_words:\n",
    "        if function in abstract:\n",
    "            abstract = abstract.replace(function, function.replace(' ', '_'))\n",
    "    abstract = tokenize(abstract)\n",
    "    return \" \".join(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain unique list of PMIDs from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while PubMed is searched and the abstracts are fetched...\n",
      "[========================] 100%\n",
      "Saved abstracts in /home/michiel/Desktop/Abstracts/\n",
      "Initial ID-list length (nested): 24\n",
      "Flattened ID-list length: 28639\n",
      "Unique ID-list length: 22020\n"
     ]
    }
   ],
   "source": [
    "# create reader object and obtain PMIDs of papers per structure\n",
    "reader = PubmedReader(query_template, structures, location)\n",
    "all_ids = reader.pmids\n",
    "\n",
    "# process and print ID list\n",
    "print 'Initial ID-list length (nested): ' + str(len(all_ids))\n",
    "all_ids = [item for sublist in all_ids for item in sublist]\n",
    "print 'Flattened ID-list length: ' + str(len(all_ids))\n",
    "all_ids = list(set(all_ids))\n",
    "print 'Unique ID-list length: ' + str(len(all_ids))\n",
    "\n",
    "# map PMIDs to indices ranging from 0 to number of IDs\n",
    "keys = range(0, len(all_ids))\n",
    "values = all_ids\n",
    "values = [str(val) for val in values]\n",
    "id_indices = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain PMIDs, titles and abstracts for every structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "struct_order_ids = []\n",
    "struct_order_titles = []\n",
    "struct_order_abstracts = []\n",
    "tokens = []\n",
    "\n",
    "# obtain IDs, titles and abstracts for every structure\n",
    "for struct in structures:\n",
    "    # create formatter object with structure\n",
    "    formatter = AbstractFormatter(location, struct)\n",
    "    # obtain ID list, title and abstract\n",
    "    id_list = formatter.get_pmids()\n",
    "    id_list = [id_entry.split(' ')[1] for id_entry in id_list] # Initial format: PMID: 12345678\n",
    "    title_list = formatter.get_titles()\n",
    "    abstract_list = formatter.get_abstracts(mode_number, location)\n",
    "    # append to list of all IDs, all titles and all abstracts\n",
    "    struct_order_ids += id_list\n",
    "    struct_order_titles += title_list\n",
    "    struct_order_abstracts += abstract_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain titles and abstracts in the order of unique ID list (in DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_ID_order_abstracts = []\n",
    "unique_ID_order_titles = []\n",
    "\n",
    "# loop unique IDs, get associated abstract and title\n",
    "for val in all_ids:\n",
    "    if val in struct_order_ids:\n",
    "        idx = struct_order_ids.index(val)\n",
    "        unique_ID_order_abstracts.append(struct_order_abstracts[idx])\n",
    "        unique_ID_order_titles.append(struct_order_titles[idx])\n",
    "    else:\n",
    "        unique_ID_order_abstracts.append(\"Missing\")\n",
    "        unique_ID_order_titles.append(\"Missing\")\n",
    "\n",
    "# save initial abstracts with IDs in DataFrame\n",
    "initial_abstracts_ID_df = pd.DataFrame({'ID': all_ids, 'Abstract': unique_ID_order_abstracts, 'Title': unique_ID_order_titles})\n",
    "initial_abstracts_ID_df.to_pickle('initial_abstract_id_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process (unique) abstracts and save in file and DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27453757 not found.\n",
      "27432102 not found.\n",
      "28153848 not found.\n",
      "27250053 not found.\n"
     ]
    }
   ],
   "source": [
    "processed_abstracts = []\n",
    "processed_titles = []\n",
    "\n",
    "# open file that will contain all abstracts and titles\n",
    "f = open('abstract_corpus.txt', 'w+')\n",
    "    \n",
    "# iterate over all unique IDs, obtain corresponding information and write it to file\n",
    "for val in all_ids:\n",
    "    # obtain title and abstract of ID (if we have it)\n",
    "    if val in struct_order_ids:\n",
    "        idx = struct_order_ids.index(val)\n",
    "        title = struct_order_titles[idx]\n",
    "        abstr = struct_order_abstracts[idx]\n",
    "        # replace function words and structure n-grams by underscore counterpart\n",
    "        abstr = preprocess(abstr, structures, function_words)\n",
    "        title = preprocess(title, structures, function_words)\n",
    "        processed_abstracts.append(abstr)\n",
    "        processed_titles.append(title)\n",
    "        # write title and abstract to file\n",
    "        f.write(title)\n",
    "        f.write('\\n\\n')\n",
    "        f.write(abstr)\n",
    "        f.write('\\n\\n\\n') # 3 newline characters separate each abstract-title combination\n",
    "    else:\n",
    "        print val + ' not found.'\n",
    "        processed_abstracts.append(\"Missing\")\n",
    "        processed_titles.append(\"Missing\")\n",
    "f.close()\n",
    "\n",
    "# save processed abstracts and titles in DataFrame\n",
    "abstracts_ID_df = pd.DataFrame({'ID': all_ids, 'Abstract': processed_abstracts, 'Title': processed_titles})\n",
    "abstracts_ID_df.to_pickle('processed_abstract_id_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame with all words and their occurence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open the abstracts and count word frequencies\n",
    "with open('abstract_corpus.txt') as f:\n",
    "    freqs = Counter(f.read().split())\n",
    "\n",
    "# remove stopwords from word index\n",
    "for sw in stopword_list:\n",
    "    del freqs[sw]\n",
    "    \n",
    "# sort vocabulary    \n",
    "sorted_x = sorted(freqs.items(), key=operator.itemgetter(1))\n",
    "sorted_x.reverse()\n",
    "\n",
    "# make DataFrame and rename columns\n",
    "all_word_occurences = pd.DataFrame(sorted_x)\n",
    "all_word_occurences.columns = ['word', 'count']\n",
    "# remove entries have too few occurences\n",
    "all_word_occurences = all_word_occurences[all_word_occurences['count'] >= mininal_count]\n",
    "\n",
    "# save vocabulary in DataFrame\n",
    "all_word_occurences.to_pickle('all_word_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame with counts for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in DataFrame with abstracts, PMIDs and titles\n",
    "abstr_df = pd.read_pickle('processed_abstract_id_map')\n",
    "# remove invalid entries\n",
    "abstr_df = abstr_df[abstr_df['Abstract'] != '1']\n",
    "abstr_df = abstr_df[abstr_df['Abstract'] != 'Missing']\n",
    "# reset indices\n",
    "abstr_df.index = range(0, len(abstr_df))\n",
    "# only select PMIDs and abstracts (not titles)\n",
    "abstr_df = abstr_df[['Abstract', 'ID']]\n",
    "abstr_df = abstr_df\n",
    "\n",
    "# split abstracts into DataFrames per abstract where the words are on individual rows\n",
    "word_dataframes = []\n",
    "for idx, row in abstr_df.iterrows():\n",
    "    tmp = pd.DataFrame({'word':row['Abstract'].split()})\n",
    "    tmp['ID'] = row['ID']\n",
    "    word_dataframes.append(tmp)\n",
    "\n",
    "# concatenate the DataFrames together    \n",
    "df = pd.concat(word_dataframes, ignore_index=True)\n",
    "# reset index\n",
    "df = df.groupby(['ID', 'word']).size().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary DataFrame and reference structures and function words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 2499342\n",
      "Individual words: 71480\n"
     ]
    }
   ],
   "source": [
    "# obtain list of all words and list of unique words\n",
    "all_words = list(df['word'])\n",
    "unique_words = df.word.unique()\n",
    "\n",
    "# print this information and create vocabulary\n",
    "print 'All words: ' + str(len(all_words))\n",
    "print 'Individual words: ' + str(len(unique_words))\n",
    "vocab = pd.DataFrame({'word': unique_words})\n",
    "\n",
    "struct_indices = []\n",
    "function_indices = []\n",
    "structures = [s.lower().replace(' ', '_') for s in structures]\n",
    "\n",
    "# obtain indices of structures and function words\n",
    "for struct in structures:\n",
    "    struct_indices.append(vocab[vocab['word'] == struct].index.tolist()[0])\n",
    "for funct in function_words:\n",
    "    function_indices.append(vocab[vocab['word'] == funct].index.tolist()[0])\n",
    "    \n",
    "# create DataFrames for structure and function word indices\n",
    "struct_df = pd.DataFrame({'word': structures, 'index': struct_indices})\n",
    "funct_df = pd.DataFrame({'word': function_words, 'index': function_indices})\n",
    "\n",
    "# change some labels, add vocab indices of words to the document counts DataFrame and filter stopwords\n",
    "df = df.rename(columns = {0:'count'})\n",
    "vocab = vocab.reset_index().set_index('word')\n",
    "df['word_id'] = df.word.map(vocab['index'].to_dict())\n",
    "df = df[~np.in1d(df.word, stopword_list)]\n",
    "\n",
    "# save DataFrames in your working directory in pickle format\n",
    "df.to_pickle('counts_per_document')\n",
    "vocab.to_pickle('vocabulary')\n",
    "funct_df.to_pickle('function_indices')\n",
    "struct_df.to_pickle('structure_indices')\n",
    "\n",
    "# df[['ID', 'word_id', 'count']].to_csv('doc_terms.csv', index=False)\n",
    "# vocab[['word', 'index']].to_csv('vocab.csv', index=False)\n",
    "# funct_df[['word', 'index']].to_csv('function_index.csv', index=False)\n",
    "# struct_df[['word', 'index']].to_csv('structure_index.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10022492</td>\n",
       "      <td>also</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10022492</td>\n",
       "      <td>arising</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10022492</td>\n",
       "      <td>basis</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10022492</td>\n",
       "      <td>brain</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10022492</td>\n",
       "      <td>broader</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10022492</td>\n",
       "      <td>capacity</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10022492</td>\n",
       "      <td>capacityconstrained</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10022492</td>\n",
       "      <td>characteristic</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10022492</td>\n",
       "      <td>characteristics</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10022492</td>\n",
       "      <td>cognitive</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10022492</td>\n",
       "      <td>consistent</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10022492</td>\n",
       "      <td>constraints</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10022492</td>\n",
       "      <td>continuously</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10022492</td>\n",
       "      <td>contrast</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10022492</td>\n",
       "      <td>cortex</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10022492</td>\n",
       "      <td>current</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10022492</td>\n",
       "      <td>decreasing</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10022492</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10022492</td>\n",
       "      <td>demonstrated</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10022492</td>\n",
       "      <td>dlpfc</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10022492</td>\n",
       "      <td>domain</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10022492</td>\n",
       "      <td>dorsolateral</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10022492</td>\n",
       "      <td>early</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10022492</td>\n",
       "      <td>effect</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10022492</td>\n",
       "      <td>evinced</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10022492</td>\n",
       "      <td>exclusively</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10022492</td>\n",
       "      <td>explained</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10022492</td>\n",
       "      <td>explorations</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10022492</td>\n",
       "      <td>fmri</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10022492</td>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499295</th>\n",
       "      <td>9989557</td>\n",
       "      <td>method</td>\n",
       "      <td>1</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499298</th>\n",
       "      <td>9989557</td>\n",
       "      <td>multiple</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499299</th>\n",
       "      <td>9989557</td>\n",
       "      <td>neither</td>\n",
       "      <td>1</td>\n",
       "      <td>4198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499301</th>\n",
       "      <td>9989557</td>\n",
       "      <td>normalized</td>\n",
       "      <td>2</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499304</th>\n",
       "      <td>9989557</td>\n",
       "      <td>often</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499306</th>\n",
       "      <td>9989557</td>\n",
       "      <td>patients</td>\n",
       "      <td>3</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499307</th>\n",
       "      <td>9989557</td>\n",
       "      <td>performance</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499308</th>\n",
       "      <td>9989557</td>\n",
       "      <td>plays</td>\n",
       "      <td>1</td>\n",
       "      <td>3157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499309</th>\n",
       "      <td>9989557</td>\n",
       "      <td>probable</td>\n",
       "      <td>1</td>\n",
       "      <td>6478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499310</th>\n",
       "      <td>9989557</td>\n",
       "      <td>provide</td>\n",
       "      <td>1</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499311</th>\n",
       "      <td>9989557</td>\n",
       "      <td>quantified</td>\n",
       "      <td>1</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499312</th>\n",
       "      <td>9989557</td>\n",
       "      <td>reallife</td>\n",
       "      <td>1</td>\n",
       "      <td>19411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499313</th>\n",
       "      <td>9989557</td>\n",
       "      <td>recall</td>\n",
       "      <td>1</td>\n",
       "      <td>4203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499314</th>\n",
       "      <td>9989557</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499315</th>\n",
       "      <td>9989557</td>\n",
       "      <td>related</td>\n",
       "      <td>1</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499316</th>\n",
       "      <td>9989557</td>\n",
       "      <td>relationship</td>\n",
       "      <td>1</td>\n",
       "      <td>1913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499317</th>\n",
       "      <td>9989557</td>\n",
       "      <td>resonance</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499319</th>\n",
       "      <td>9989557</td>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499320</th>\n",
       "      <td>9989557</td>\n",
       "      <td>role</td>\n",
       "      <td>1</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499321</th>\n",
       "      <td>9989557</td>\n",
       "      <td>semistructured</td>\n",
       "      <td>1</td>\n",
       "      <td>13891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499322</th>\n",
       "      <td>9989557</td>\n",
       "      <td>strong</td>\n",
       "      <td>1</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499323</th>\n",
       "      <td>9989557</td>\n",
       "      <td>structures</td>\n",
       "      <td>1</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499324</th>\n",
       "      <td>9989557</td>\n",
       "      <td>study</td>\n",
       "      <td>1</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499325</th>\n",
       "      <td>9989557</td>\n",
       "      <td>suggests</td>\n",
       "      <td>1</td>\n",
       "      <td>1465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499326</th>\n",
       "      <td>9989557</td>\n",
       "      <td>surrounding</td>\n",
       "      <td>1</td>\n",
       "      <td>3444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499327</th>\n",
       "      <td>9989557</td>\n",
       "      <td>temporal</td>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499334</th>\n",
       "      <td>9989557</td>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499335</th>\n",
       "      <td>9989557</td>\n",
       "      <td>volume</td>\n",
       "      <td>3</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499336</th>\n",
       "      <td>9989557</td>\n",
       "      <td>volumes</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499341</th>\n",
       "      <td>9989557</td>\n",
       "      <td>work</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1973781 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                 word  count  word_id\n",
       "1        10022492                 also      1        1\n",
       "5        10022492              arising      1        5\n",
       "7        10022492                basis      1        7\n",
       "9        10022492                brain      1        9\n",
       "10       10022492              broader      1       10\n",
       "11       10022492             capacity      7       11\n",
       "12       10022492  capacityconstrained      3       12\n",
       "13       10022492       characteristic      1       13\n",
       "14       10022492      characteristics      2       14\n",
       "15       10022492            cognitive      1       15\n",
       "16       10022492           consistent      1       16\n",
       "17       10022492          constraints      1       17\n",
       "18       10022492         continuously      1       18\n",
       "19       10022492             contrast      1       19\n",
       "20       10022492               cortex      2       20\n",
       "21       10022492              current      1       21\n",
       "22       10022492           decreasing      1       22\n",
       "23       10022492          demonstrate      1       23\n",
       "24       10022492         demonstrated      1       24\n",
       "26       10022492                dlpfc      3       26\n",
       "27       10022492               domain      1       27\n",
       "28       10022492         dorsolateral      1       28\n",
       "29       10022492                early      1       29\n",
       "30       10022492               effect      1       30\n",
       "31       10022492              evinced      1       31\n",
       "32       10022492          exclusively      1       32\n",
       "33       10022492            explained      1       33\n",
       "34       10022492         explorations      1       34\n",
       "35       10022492                 fmri      1       35\n",
       "37       10022492           functional      1       37\n",
       "...           ...                  ...    ...      ...\n",
       "2499295   9989557               method      1      758\n",
       "2499298   9989557             multiple      1      354\n",
       "2499299   9989557              neither      1     4198\n",
       "2499301   9989557           normalized      2      593\n",
       "2499304   9989557                often      1       77\n",
       "2499306   9989557             patients      3      203\n",
       "2499307   9989557          performance      1       83\n",
       "2499308   9989557                plays      1     3157\n",
       "2499309   9989557             probable      1     6478\n",
       "2499310   9989557              provide      1      469\n",
       "2499311   9989557           quantified      1      762\n",
       "2499312   9989557             reallife      1    19411\n",
       "2499313   9989557               recall      1     4203\n",
       "2499314   9989557           regression      1     1131\n",
       "2499315   9989557              related      1      473\n",
       "2499316   9989557         relationship      1     1913\n",
       "2499317   9989557            resonance      1       94\n",
       "2499319   9989557                right      1      210\n",
       "2499320   9989557                 role      1      988\n",
       "2499321   9989557       semistructured      1    13891\n",
       "2499322   9989557               strong      1     1746\n",
       "2499323   9989557           structures      1      491\n",
       "2499324   9989557                study      1      309\n",
       "2499325   9989557             suggests      1     1465\n",
       "2499326   9989557          surrounding      1     3444\n",
       "2499327   9989557             temporal      1      228\n",
       "2499334   9989557                  use      1      997\n",
       "2499335   9989557               volume      3      240\n",
       "2499336   9989557              volumes      1      241\n",
       "2499341   9989557                 work      1      999\n",
       "\n",
       "[1973781 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: NONE OF THE SYNONYMS SEEM TO BE IN THERE   \n",
    "\n",
    "# process synonyms\n",
    "synonyms = []\n",
    "syns = [syn.lower() for syn in info.synonyms]\n",
    "for syn in syns:\n",
    "    if not syn == 'none':\n",
    "        for elem in syn.split(','):\n",
    "            synonyms.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Four' appears 1602 times\n",
      "'the' appears 0 times\n",
      "There are 4655788 total words\n",
      "The 5 most common words are [('the', 245840), ('of', 175384), ('and', 174981), ('in', 163676), ('to', 81106), ('with', 78024), ('a', 68997), ('was', 36921), ('patients', 35713), ('were', 35611), ('for', 30929), ('that', 30211), ('brain', 28244), ('is', 25266), ('on', 21488), ('by', 20972), ('we', 20434), ('as', 19962), ('imaging', 18046), ('this', 17856), ('cortex', 15174), ('between', 14927), ('an', 14524), ('functional', 14425), ('study', 14350), ('amygdala', 14173), ('these', 14028), ('results', 13559), ('from', 13505), ('or', 13488)]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: ALTERNATIVE WAY TO OBTAIN FREQUENCIES\n",
    "\n",
    "import collections\n",
    "\n",
    "with open('abstract_corpus.txt') as f:\n",
    "    c = collections.Counter(f.read().split())\n",
    "\n",
    "print \"'Four' appears %d times\"%c['four']\n",
    "print \"'the' appears %d times\"%c['subthalamic_nucleus']\n",
    "print \"There are %d total words\"%sum(c.values())\n",
    "print \"The 5 most common words are\", c.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
